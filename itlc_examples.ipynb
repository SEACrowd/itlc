{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3db30e2-8d40-4312-8e90-c17004b69571",
   "metadata": {},
   "source": [
    "## Using provided LDA weight and language vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db3954c-7ab1-4827-8096-0069c390fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code = {\n",
    "    \"ace_Arab\": 0,\n",
    "    \"ace_Latn\": 1,\n",
    "    \"acm_Arab\": 2,\n",
    "    \"acq_Arab\": 3,\n",
    "    \"aeb_Arab\": 4,\n",
    "    \"afr_Latn\": 5,\n",
    "    \"ajp_Arab\": 6,\n",
    "    \"aka_Latn\": 7,\n",
    "    \"als_Latn\": 8,\n",
    "    \"amh_Ethi\": 9,\n",
    "    \"apc_Arab\": 10,\n",
    "    \"arb_Arab\": 11,\n",
    "    \"arb_Latn\": 12,\n",
    "    \"ars_Arab\": 13,\n",
    "    \"ary_Arab\": 14,\n",
    "    \"arz_Arab\": 15,\n",
    "    \"asm_Beng\": 16,\n",
    "    \"ast_Latn\": 17,\n",
    "    \"awa_Deva\": 18,\n",
    "    \"ayr_Latn\": 19,\n",
    "    \"azb_Arab\": 20,\n",
    "    \"azj_Latn\": 21,\n",
    "    \"bak_Cyrl\": 22,\n",
    "    \"bam_Latn\": 23,\n",
    "    \"ban_Latn\": 24,\n",
    "    \"bel_Cyrl\": 25,\n",
    "    \"bem_Latn\": 26,\n",
    "    \"ben_Beng\": 27,\n",
    "    \"bho_Deva\": 28,\n",
    "    \"bjn_Arab\": 29,\n",
    "    \"bjn_Latn\": 30,\n",
    "    \"bod_Tibt\": 31,\n",
    "    \"bos_Latn\": 32,\n",
    "    \"bug_Latn\": 33,\n",
    "    \"bul_Cyrl\": 34,\n",
    "    \"cat_Latn\": 35,\n",
    "    \"ceb_Latn\": 36,\n",
    "    \"ces_Latn\": 37,\n",
    "    \"cjk_Latn\": 38,\n",
    "    \"ckb_Arab\": 39,\n",
    "    \"crh_Latn\": 40,\n",
    "    \"cym_Latn\": 41,\n",
    "    \"dan_Latn\": 42,\n",
    "    \"deu_Latn\": 43,\n",
    "    \"dik_Latn\": 44,\n",
    "    \"dyu_Latn\": 45,\n",
    "    \"dzo_Tibt\": 46,\n",
    "    \"ell_Grek\": 47,\n",
    "    \"eng_Latn\": 48,\n",
    "    \"epo_Latn\": 49,\n",
    "    \"est_Latn\": 50,\n",
    "    \"eus_Latn\": 51,\n",
    "    \"ewe_Latn\": 52,\n",
    "    \"fao_Latn\": 53,\n",
    "    \"fij_Latn\": 54,\n",
    "    \"fin_Latn\": 55,\n",
    "    \"fon_Latn\": 56,\n",
    "    \"fra_Latn\": 57,\n",
    "    \"fur_Latn\": 58,\n",
    "    \"fuv_Latn\": 59,\n",
    "    \"gaz_Latn\": 60,\n",
    "    \"gla_Latn\": 61,\n",
    "    \"gle_Latn\": 62,\n",
    "    \"glg_Latn\": 63,\n",
    "    \"grn_Latn\": 64,\n",
    "    \"guj_Gujr\": 65,\n",
    "    \"hat_Latn\": 66,\n",
    "    \"hau_Latn\": 67,\n",
    "    \"heb_Hebr\": 68,\n",
    "    \"hin_Deva\": 69,\n",
    "    \"hne_Deva\": 70,\n",
    "    \"hrv_Latn\": 71,\n",
    "    \"hun_Latn\": 72,\n",
    "    \"hye_Armn\": 73,\n",
    "    \"ibo_Latn\": 74,\n",
    "    \"ilo_Latn\": 75,\n",
    "    \"ind_Latn\": 76,\n",
    "    \"isl_Latn\": 77,\n",
    "    \"ita_Latn\": 78,\n",
    "    \"jav_Latn\": 79,\n",
    "    \"jpn_Jpan\": 80,\n",
    "    \"kab_Latn\": 81,\n",
    "    \"kac_Latn\": 82,\n",
    "    \"kam_Latn\": 83,\n",
    "    \"kan_Knda\": 84,\n",
    "    \"kas_Arab\": 85,\n",
    "    \"kas_Deva\": 86,\n",
    "    \"kat_Geor\": 87,\n",
    "    \"kaz_Cyrl\": 88,\n",
    "    \"kbp_Latn\": 89,\n",
    "    \"kea_Latn\": 90,\n",
    "    \"khk_Cyrl\": 91,\n",
    "    \"khm_Khmr\": 92,\n",
    "    \"kik_Latn\": 93,\n",
    "    \"kin_Latn\": 94,\n",
    "    \"kir_Cyrl\": 95,\n",
    "    \"kmb_Latn\": 96,\n",
    "    \"kmr_Latn\": 97,\n",
    "    \"knc_Arab\": 98,\n",
    "    \"knc_Latn\": 99,\n",
    "    \"kon_Latn\": 100,\n",
    "    \"kor_Hang\": 101,\n",
    "    \"lao_Laoo\": 102,\n",
    "    \"lij_Latn\": 103,\n",
    "    \"lim_Latn\": 104,\n",
    "    \"lin_Latn\": 105,\n",
    "    \"lit_Latn\": 106,\n",
    "    \"lmo_Latn\": 107,\n",
    "    \"ltg_Latn\": 108,\n",
    "    \"ltz_Latn\": 109,\n",
    "    \"lua_Latn\": 110,\n",
    "    \"lug_Latn\": 111,\n",
    "    \"luo_Latn\": 112,\n",
    "    \"lus_Latn\": 113,\n",
    "    \"lvs_Latn\": 114,\n",
    "    \"mag_Deva\": 115,\n",
    "    \"mai_Deva\": 116,\n",
    "    \"mal_Mlym\": 117,\n",
    "    \"mar_Deva\": 118,\n",
    "    \"min_Arab\": 119,\n",
    "    \"min_Latn\": 120,\n",
    "    \"mkd_Cyrl\": 121,\n",
    "    \"mlt_Latn\": 122,\n",
    "    \"mni_Beng\": 123,\n",
    "    \"mos_Latn\": 124,\n",
    "    \"mri_Latn\": 125,\n",
    "    \"mya_Mymr\": 126,\n",
    "    \"nld_Latn\": 127,\n",
    "    \"nno_Latn\": 128,\n",
    "    \"nob_Latn\": 129,\n",
    "    \"npi_Deva\": 130,\n",
    "    \"nso_Latn\": 131,\n",
    "    \"nus_Latn\": 132,\n",
    "    \"nya_Latn\": 133,\n",
    "    \"oci_Latn\": 134,\n",
    "    \"ory_Orya\": 135,\n",
    "    \"pag_Latn\": 136,\n",
    "    \"pan_Guru\": 137,\n",
    "    \"pap_Latn\": 138,\n",
    "    \"pbt_Arab\": 139,\n",
    "    \"pes_Arab\": 140,\n",
    "    \"plt_Latn\": 141,\n",
    "    \"pol_Latn\": 142,\n",
    "    \"por_Latn\": 143,\n",
    "    \"prs_Arab\": 144,\n",
    "    \"quy_Latn\": 145,\n",
    "    \"ron_Latn\": 146,\n",
    "    \"run_Latn\": 147,\n",
    "    \"rus_Cyrl\": 148,\n",
    "    \"sag_Latn\": 149,\n",
    "    \"san_Deva\": 150,\n",
    "    \"sat_Olck\": 151,\n",
    "    \"scn_Latn\": 152,\n",
    "    \"shn_Mymr\": 153,\n",
    "    \"sin_Sinh\": 154,\n",
    "    \"slk_Latn\": 155,\n",
    "    \"slv_Latn\": 156,\n",
    "    \"smo_Latn\": 157,\n",
    "    \"sna_Latn\": 158,\n",
    "    \"snd_Arab\": 159,\n",
    "    \"som_Latn\": 160,\n",
    "    \"sot_Latn\": 161,\n",
    "    \"spa_Latn\": 162,\n",
    "    \"srd_Latn\": 163,\n",
    "    \"srp_Cyrl\": 164,\n",
    "    \"ssw_Latn\": 165,\n",
    "    \"sun_Latn\": 166,\n",
    "    \"swe_Latn\": 167,\n",
    "    \"swh_Latn\": 168,\n",
    "    \"szl_Latn\": 169,\n",
    "    \"tam_Taml\": 170,\n",
    "    \"taq_Latn\": 171,\n",
    "    \"taq_Tfng\": 172,\n",
    "    \"tat_Cyrl\": 173,\n",
    "    \"tel_Telu\": 174,\n",
    "    \"tgk_Cyrl\": 175,\n",
    "    \"tgl_Latn\": 176,\n",
    "    \"tha_Thai\": 177,\n",
    "    \"tir_Ethi\": 178,\n",
    "    \"tpi_Latn\": 179,\n",
    "    \"tsn_Latn\": 180,\n",
    "    \"tso_Latn\": 181,\n",
    "    \"tuk_Latn\": 182,\n",
    "    \"tum_Latn\": 183,\n",
    "    \"tur_Latn\": 184,\n",
    "    \"twi_Latn\": 185,\n",
    "    \"tzm_Tfng\": 186,\n",
    "    \"uig_Arab\": 187,\n",
    "    \"ukr_Cyrl\": 188,\n",
    "    \"umb_Latn\": 189,\n",
    "    \"urd_Arab\": 190,\n",
    "    \"uzn_Latn\": 191,\n",
    "    \"vec_Latn\": 192,\n",
    "    \"vie_Latn\": 193,\n",
    "    \"war_Latn\": 194,\n",
    "    \"wol_Latn\": 195,\n",
    "    \"xho_Latn\": 196,\n",
    "    \"ydd_Hebr\": 197,\n",
    "    \"yor_Latn\": 198,\n",
    "    \"yue_Hant\": 199,\n",
    "    \"zho_Hans\": 200,\n",
    "    \"zho_Hant\": 201,\n",
    "    \"zsm_Latn\": 202,\n",
    "    \"zul_Latn\": 203,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbf420f-e70e-47b9-9f51-89a4e015d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itlc import ITLC\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load qwen model\n",
    "model_name = \"/root/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5bbc77-6bb7-44a3-9b1c-b1424f8f4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ITLC model\n",
    "itlc = ITLC(model=model, tokenizer=tokenizer, lda_model_path = \"./lda_model.pkl\", langvec_path = \"./langvec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b44a8e-e965-4a6e-a3a6-b4d6d9fbd833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['こんにちは！(おはよう)、こんばん你？（とことく）すみませんが…申し訳ありませんでした。あなたの問題を聞かせていただけますでしょうか？\\n\\n私は「Q文」として、「心の晴']\n"
     ]
    }
   ],
   "source": [
    "# Set the target language\n",
    "source_language = \"eng_Latn\"\n",
    "target_language = \"jpn_Jpan\"\n",
    "\n",
    "## apply chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate text in the target language\n",
    "generated_text = itlc.generate(prompt=text, src_id=lang_code[source_language], tgt_id= lang_code[target_language], max_new_tokens=50)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a6331e-2e0a-4bcb-80d2-7842db5e3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sebaya! Sama-samankit saya juga sibuk berkomunikasi dan bekerja dalam program AI yang kompleks seperti itu.\\n\\nDalam hal kenyataannya, saat ini Anda sedang mengobrol dengan sebag', 'Tentu saya boleh memberikan jawaban yang lebih umum:\\n\\n\"2\"\\n\\nDengan demikian:\\n- \"tak ada\"\\n   - Dalam konteks matematika dan komputer\\n\\nSebenarnya kita tidak bisa']\n"
     ]
    }
   ],
   "source": [
    "# Set the target language\n",
    "source_language = \"eng_Latn\"\n",
    "target_language = \"ind_Latn\"\n",
    "\n",
    "## apply chat template\n",
    "message_batch = [\n",
    "    \"How are you?\",\n",
    "    \"can you answer the result of 1+1?\",\n",
    "]\n",
    "\n",
    "message_batch = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        for prompt in message_batch\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    message_batch,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "# Generate text in the target language\n",
    "generated_text = itlc.generate(prompt=text, src_id=lang_code[source_language], tgt_id= lang_code[target_language], max_new_tokens=50)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ccab3b0-fcd6-4dee-b1de-90fb7abdf76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I am an artificial intelligence language model and I don't have personal experiences or emotions like humans do.\\nHowever, as you know from my training data on the internet to answer your questions about specific topics related to AI research over time with respect for this limitation\", \"I'm sorry that you're not aware of the location information for Thailand at this time (2019). I'll try to find out what it is based on my current knowledge.\\nCan anyone else tell me where in China? If so:\"]\n"
     ]
    }
   ],
   "source": [
    "# Set the target language\n",
    "source_language = \"tha_Thai\"\n",
    "target_language = \"eng_Latn\"\n",
    "\n",
    "## apply chat template\n",
    "message_batch = [\n",
    "    \"คุณเป็นอย่างไร\",\n",
    "    \"คุณบอกฉันได้ไหมว่าประเทศไทยอยู่ที่ไหน\",\n",
    "]\n",
    "\n",
    "message_batch = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        for prompt in message_batch\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    message_batch,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "# Generate text in the target language\n",
    "generated_text = itlc.generate(prompt=text, src_id=lang_code[source_language], tgt_id= lang_code[target_language], max_new_tokens=50)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3ea97-64aa-452e-a95a-9b6679451072",
   "metadata": {},
   "source": [
    "## Using Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc79325-b5c1-4651-b4ce-2deb771766e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itlc import ITLC\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be3db65-483f-4dcd-b294-9a82dc936bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load qwen model\n",
    "model_name = \"/root/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da95ff7d-2f73-4a50-8ce7-868296852075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize itlc\n",
    "itlc = ITLC(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c43db6c-34c4-41a1-b1d6-3364108d3274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing languages:   0%|          | 0/204 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Processing languages: 100%|██████████| 204/204 [07:36<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# load dataset from hf\n",
    "ds = load_dataset(\"/root/multilingual/flores200\", split=\"validation[:100]\")\n",
    "language_pairs = list(ds.features) #you can get the corresponding language ID here\n",
    "# extract embedding\n",
    "embeddings, labels = itlc.latent_extraction(dataset=ds, language_pairs=language_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a0e197-9416-4ccc-8835-a9207db82c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.82%\n"
     ]
    }
   ],
   "source": [
    "# train lda model\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
    "lda_model, language_vectors = itlc.language_vector_extraction(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e08758a-517e-4dc6-a658-45e6a44fb708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/langvec.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained LDA model to disk\n",
    "lda_model_save_path= \"./lda_model.pkl\"\n",
    "langvec_save_path = \"./langvec.pkl\"\n",
    "\n",
    "joblib.dump(lda_model, lda_model_save_path)\n",
    "joblib.dump(language_vectors, langvec_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35068ffe-0005-4025-81b8-22caca5f862b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' - 안녕하세요\\n안녕 하셔서(おはようございます) 반갑습니다. \\xa0~내가하는 비밀, ~자기 소개문,\\n\\nあなたの 이름은? \\n\\n이름 : \\n<공지사항> (', ' 한국어, English\\n\\n이제는 문화가 대량으로 이슈를 낸 건데요 \\n\\n韓말: ゼ네아시아\\nEnglish : KOREA (中国)의 첫 번째 옆지우']\n"
     ]
    }
   ],
   "source": [
    "# load LDA and language vector\n",
    "itlc = ITLC(model=model, tokenizer=tokenizer, lda_model_path = \"./lda_model.pkl\", langvec_path = \"./langvec.pkl\")\n",
    "\n",
    "# Set the target language\n",
    "source_language = \"eng_Latn\"\n",
    "target_language = \"kor_Hang\"\n",
    "\n",
    "text = [\"please introduce yourself\", \"can you tell me some well known culture in korea?\"\n",
    "]\n",
    "\n",
    "# Generate text in the target language\n",
    "generated_text = itlc.generate(prompt=text, src_id=lang_code[source_language], tgt_id= lang_code[target_language], max_new_tokens=50)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf9b0e-8887-44f0-b253-b3b13ad486d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
